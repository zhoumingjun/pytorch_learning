from torch.utils.data import Dataset, DataLoader
from os import path
import unicodedata
import torch
import string

import torch.nn.functional as F
import re

import numpy as np


class NamesDataset(Dataset):
    def __init__(self, data_path):

        inputs = []
        all_letters = set([])
        with open(data_path) as f:
            while True:
                seq = f.read(10)
                if not seq or len(seq) != 10:
                    print("End of file")
                    break

                all_letters = all_letters | set(seq)
                inputs.append(seq)

        self.inputs = inputs
        self.all_letters = [x for x in all_letters]
        self.n_letters = len(self.all_letters)

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        return self.inputs[idx]

    def getAllLetters(self):
        return self.all_letters

    def getNLetters(self):
        return self.n_letters

    def name2tensor(self, s):
        tensor = torch.zeros(len(s), self.n_letters)
        for li, letter in enumerate(s):
            tensor[li][self.all_letters.index(letter)] = 1
        return tensor

    def name2Class(self, s):
        return [self.all_letters.index(x) for x in s]

    def names2tensor(self, s):
        batch = len(s)

        seq = len(s[0])

        tensor = torch.zeros(seq, batch, self.n_letters)
        for x, name in enumerate(s):
            for y, ch in enumerate(name):
                tensor[y][x][self.all_letters.index(ch)] = 1
        return tensor

    def names2Class(self, s):

        batch = len(s)
        seq = len(s[0])
        tensor = torch.zeros(seq, batch, dtype=torch.long)

        for x, name in enumerate(s):
            for y, ch in enumerate(name):
                tensor[y][x] = self.all_letters.index(ch)
        return tensor


dataset = NamesDataset("./data/code.txt")
print("fiction length:", len(dataset) * 1000)
print("letters stat:", dataset.getNLetters(), dataset.getAllLetters())
print("name2class", dataset.name2Class("iam"))
print("name2tensor", dataset.name2tensor("Iam"))

dataloader = DataLoader(dataset, batch_size=100, shuffle=True, num_workers=4, drop_last=False)


class FictionGenerator(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(FictionGenerator, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)
        self.linear = torch.nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden):
        x, hidden = self.lstm(x, hx=hidden)
        x = x.view(-1, self.hidden_size)
        x = self.linear(x)
        return F.log_softmax(x, dim=1), hidden


input_size = dataset.n_letters
hidden_size = 500
output_size = dataset.n_letters

# define model/optimizer/criterion
model = FictionGenerator(input_size, hidden_size, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.NLLLoss()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model = model.to(device)


def sample(start=1, limit=1000):
    with torch.no_grad():  # no need to track history in sampling
        letter = dataset.all_letters[start]
        fiction = letter

        input = dataset.name2tensor(letter).view(1, 1, input_size).to(device)
        hidden = (torch.zeros(1, 1, hidden_size).to(device), torch.zeros(1, 1, hidden_size).to(device))

        for i in range(limit):
            output, hidden = model(input, hidden)
            p = torch.exp(output).squeeze().cpu().numpy()

            # random choice according to prob
            ix = np.random.choice(range(dataset.n_letters), p=p)

            # max prob
            # topv, topi = output.topk(1)
            # ix = topi[0][0]
            #
            # print(ix, topi)

            letter = dataset.all_letters[ix]
            fiction += letter

            input = dataset.name2tensor(letter).view(-1, 1, input_size).to(device)

        print("<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<")
        print(fiction)
        print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
        return fiction


for epoch in range(1000):

    for i_batch, batch in enumerate(dataloader):
        optimizer.zero_grad()

        # prepare data
        input = [x[:-1] for x in batch]
        label = [x[1:] for x in batch]
        inputs = dataset.names2tensor(input).to(device)
        labels = dataset.names2Class(label).view(-1).to(device)

        # train
        output, _ = model(inputs, None)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()

    print("epoch {} i_batch {} loss {}".format(epoch, i_batch, loss))
    sample()
import torch
import numpy as np

from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence

input_size = 2
hidden_size= 5
num_layers = 1
nClasses = 10
nSamples = 10

a = torch.ones(3, input_size)
b = torch.ones(5, input_size)
c = torch.ones(7, input_size)

# pad
pad = pad_sequence([c,b,a])
print("pad result", pad.size())

# pack
pack = pack_sequence([c,b,a])
print("pack result:", pack.data.size(), pack.batch_sizes)

# pack_padded
pack_padded = pack_padded_sequence(pad, [7,5,3])
print("pack_padded result:", pack_padded.data.size(), pack_padded.batch_sizes)

# pad_packed
pad_packed_data, pad_packed_lengths = pad_packed_sequence(pack)
print("pad_packed result:", pad_packed_data.size() ,pad_packed_lengths)

# pattern

"""
prepare data/model/indices
"""

# data
inputs = []
targets = []
for idx in range(nSamples):
    # set random len of input , and set the len as target
    # input: ones(len, input_size)
    # target: len
    len = np.random.randint(nSamples)+1
    sample = torch.ones(len, input_size)
    inputs.append(sample)
    targets.append(len)

# model
model = torch.nn.LSTM(input_size, hidden_size, num_layers)
demo = torch.ones(10,1,  input_size)
print("sample sequence result", model(demo)[0])

# indices
sample_length = [x.size(0) for x in inputs]
_, indices_sorted = torch.sort(torch.LongTensor(sample_length), descending=True)
_, indices_restore = torch.sort(indices_sorted)

print("sample length:", sample_length)

"""
option1:
pre-process inputs
sort (inputs)-> pack(inputs) -> rnn -> unpack -> unsort(outputs)

targets <-> outputs  
"""
print("option1")

 

# sort inputs
inputs_sorted = [inputs[x] for x in indices_sorted]

# pack inputs
pack = pack_sequence(inputs_sorted)

# rnn ...
outputs, hidden = model(pack)

# unpack
output_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)
last_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]

# unsort
unsorted_last_state = last_state[indices_restore,:]
print([(tup[0].size(0), tup[1], tup[2]) for tup in   zip(inputs, targets, unsorted_last_state)])

"""
option2 
pre-process (inputs, targets)
sort (inputs, targets)-> pack(inputs) -> rnn -> unpack

targets(sorted) <--> outputs  
"""

print("option2")
batch = list(zip(inputs, targets))

# sort inputs
batch_sorted = [batch[x] for x in indices_sorted]

# pack inputs
pack = pack_sequence([tup[0] for tup in batch_sorted])

# rnn ...
outputs, hidden = model(pack)

# unpack
output_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)
last_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]

print([(tup[0][0].size(0), tup[0][1], tup[1]) for tup in zip(batch_sorted, last_state)])


import glob
import os
from torch.utils.data import Dataset, DataLoader
from os import path
import unicodedata
import torch
import string

import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence

all_letters = string.ascii_letters + " .,;'"
n_letters = len(all_letters)

# dataset
class NamesDataset(Dataset):
    def __init__(self, data_dir, transforms=[]):
        self.data_dir = data_dir

        all_langs = []
        inputs = []
        labels = []

        for filepath in glob.glob(path.join(data_dir, "*")):
            lang = os.path.splitext(os.path.basename(filepath))[0]
            if not lang in all_langs:
                all_langs.append(lang)

            label = all_langs.index(lang)

            with open(filepath) as f:
                lines = f.readlines()
                inputs += [line.strip() for line in lines]
                labels += [label] * len(lines)

        self.all_langs = all_langs
        self.inputs = inputs
        self.labels = labels
        self.transforms = transforms

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):

        item = self.inputs[idx]
        for transform in self.transforms:
            item = transform(item)

        return item, self.labels[idx]

    def getLangs(self):
        return self.all_langs


# transform
class UnicodeToAscii(object):
    def __init__(self, letters):
        self.letters = letters

    def __call__(self, s):
        return ''.join(
            c for c in unicodedata.normalize('NFD', s)
            if unicodedata.category(c) != 'Mn'
            and c in self.letters
        )

# ascii name to tenser
def name2tensor(s):
    tensor = torch.zeros(len(s), n_letters)
    for li, letter in enumerate(s):
        tensor[li][all_letters.index(letter)] = 1
    return tensor


def categoryFromOutput(output):
    top_n, top_i = output.topk(1)
    category_i = top_i[0].item()
    return all_langs[category_i], category_i


def predict(s):
    with torch.no_grad():
        ascii = unicode2ascii(s)
        input = name2tensor(ascii)
        pack = pack_sequence([input])
        output = model(pack)
        return categoryFromOutput(output)

class NamesClassifier(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NamesClassifier, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)
        self.linear = torch.nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x, hidden = self.lstm(x)
        output_unpacked, unpack_outputs_length = pad_packed_sequence(x)

        seqs = unpack_outputs_length - 1
        batch = [x for x in range(len(unpack_outputs_length))]
        last_state = output_unpacked[seqs, batch, :].view(-1, self.hidden_size)

        x = self.linear(last_state)
        return F.log_softmax(x, dim=1)


unicode2ascii = UnicodeToAscii(all_letters)

# define dataset and dataloader
namesDataset = NamesDataset('./data/names', transforms=[unicode2ascii])
dataloader = DataLoader(namesDataset, batch_size=100, shuffle=True, num_workers=4, drop_last=True)

# hyper parameters
input_size = n_letters
hidden_size = 50
all_langs = namesDataset.getLangs()
output_size = len(all_langs)

# define model/optimizer/criterion
model = NamesClassifier(input_size, hidden_size, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.NLLLoss()

print("all_langs", all_langs)

for epoch in range(10):

    # train
    loss_sum = 0;
    nRound = 0
    for i_batch, batch in enumerate(dataloader):

        # zero
        optimizer.zero_grad()

        inputs, labels = batch
        # pre-process
        inputs = [name2tensor(name) for name in inputs]

        inputs_length = [x.size(0) for x in inputs]
        _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)
        _, indices_restore = torch.sort(indices_sorted)

        # sort
        inputs_sorted = [inputs[x] for x in indices_sorted]
        labels_sorted = labels[indices_sorted]

        # pack inputs
        pack = pack_sequence(inputs_sorted)

        # rnn
        outputs = model(pack)

        # loss/bp/step
        loss = criterion(outputs, labels_sorted)

        loss.backward()
        optimizer.step()

        loss_sum += loss
        nRound += 1
        if i_batch % 50 == 0:
            print("epoch {} i_batch {} loss {}".format(epoch, i_batch, loss_sum / nRound))

    # validate
    with torch.no_grad():
        acc = 0
        for i_batch, batch in enumerate(dataloader):
            inputs, labels = batch
            # pre-process
            inputs = [name2tensor(name) for name in inputs]

            inputs_length = [x.size(0) for x in inputs]
            _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)
            _, indices_restore = torch.sort(indices_sorted)

            # sort
            inputs_sorted = [inputs[x] for x in indices_sorted]
            labels_sorted = labels[indices_sorted]

            # pack inputs
            pack = pack_sequence(inputs_sorted)

            # rnn
            outputs = model(pack)

            top_v, topi = torch.topk(outputs, 1)
            acc += (topi.view(1, -1) == labels_sorted).sum().item()

    print("epoch {} acc:{}/{} ".format(epoch, acc, len(namesDataset)))

# do some preidct
for i_batch, batch in enumerate(dataloader):
    input, label = batch
    for idx, input in enumerate(input):
        lang, lang_id = predict(input)
        print("input {}, label {}, predict {}, result: {}".format(
            input, all_langs[label[idx].item()],
            lang,
            lang_id == label[idx].item()))

    break
import torch

input_size = 10
hidden_size = 20
num_layers = 1

# model
model = torch.nn.LSTM(input_size, hidden_size, num_layers)

# data
input = torch.randn(4, 4, 10)

# option1: sequence
output, hidden = model(input)

# option2: step by step
input_0 = input[:, 0, :].view(4,1,10)
input_1 = input[:, 1, :].view(4,1,10)
input_2 = input[:, 2, :].view(4,1,10)
input_3 = input[:, 3, :].view(4,1,10)

output_0, hidden_0 = model(input_0)
output_1, hidden_1 = model(input_1)
output_2, hidden_2 = model(input_2)
output_3, hidden_3 = model(input_3)


print((output[-1][0]- output_0[-1][0]).sum())
print((output[-1][1]- output_1[-1][0]).sum())
print((output[-1][2]- output_2[-1][0]).sum())
print((output[-1][3]- output_3[-1][0]).sum())

import torch

input_size = 10
hidden_size = 20
num_layers = 1

# model
model = torch.nn.LSTM(input_size, hidden_size, num_layers)

# data
input = torch.ones(4, 1, 10)

# option1: sequence
output, hidden = model(input)

# option2: step by step
input_0 = input[0,:,:].view(1,1,10)
input_1 = input[1,:,:].view(1,1,10)
input_2 = input[2,:,:].view(1,1,10)
input_3 = input[3,:,:].view(1,1,10)

output_0, hidden_0 = model(input_0)
output_1, hidden_1 = model(input_1, hidden_0)
output_2, hidden_2 = model(input_2, hidden_1)
output_3, hidden_3 = model(input_3, hidden_2)


print(hidden)
print(output)
print(hidden_0, hidden_1, hidden_2,hidden_3)
print(output_0, output_1, output_2,output_3)


# compare option1 & option2
print ((output[0]==output_0).sum().item() == hidden_size)
print ((output[1]==output_1).sum().item() == hidden_size)
print ((output[2]==output_2).sum().item() == hidden_size)
print ((output[3]==output_3).sum().item() == hidden_size)

"""
True
True
True
True
"""
# relation between hidden & output
print ((output[0]==hidden_0[0][-1]).sum().item() == hidden_size)
print ((output[1]==hidden_1[0][-1]).sum().item() == hidden_size)
print ((output[2]==hidden_2[0][-1]).sum().item() == hidden_size)
print ((output[3]==hidden_3[0][-1]).sum().item() == hidden_size)
"""
True
True
True
True
"""from torch.utils.data import Dataset, DataLoader
from os import path
import unicodedata
import torch
import string

import torch.nn.functional as F

all_letters = string.ascii_letters + " .,;'-"
n_letters = len(all_letters)

# ascii name to tenser
def name2tensor(s):
    tensor = torch.zeros(len(s), n_letters)
    for li, letter in enumerate(s):
        tensor[li][all_letters.index(letter)] = 1
    return tensor

# dataset
class NamesDataset(Dataset):
    def __init__(self, data_path, transforms=[]):
        inputs = []
        filepath = path.join(data_path)

        with open(filepath) as f:
            lines = f.readlines()
            inputs += [line.strip() for line in lines]

        self.inputs = inputs
        self.transforms = transforms

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        input = self.inputs[idx]
        for transform in self.transforms:
            input = transform(input)
        return input


class UnicodeToAscii(object):
    def __init__(self, letters):
        self.letters = letters

    def __call__(self, s):
        return ''.join(
            c for c in unicodedata.normalize('NFD', s)
            if unicodedata.category(c) != 'Mn'
            and c in self.letters
        )


namesDataset = NamesDataset('./data/names/English.txt', transforms=[UnicodeToAscii(all_letters)])


class NamesClassifier(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NamesClassifier, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)
        self.linear = torch.nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden):
        x, hidden = self.lstm(x, hx=hidden)
        x = x.view(-1, hidden_size)
        x = self.linear(x)
        return F.log_softmax(x, dim=1), hidden



dataloader = DataLoader(namesDataset, batch_size=1, shuffle=True, num_workers=4, drop_last=True)

input_size = n_letters
hidden_size = 50
output_size = input_size

# define model/optimizer/criterion
model = NamesClassifier(input_size, hidden_size, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.NLLLoss()

max_length = 20
def sample(start_letter='A'):
    with torch.no_grad():  # no need to track history in sampling
        input = name2tensor(start_letter).view(-1, 1, input_size)
        output_name = start_letter
        hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1,1,hidden_size))

        for i in range(max_length):
            output, hidden = model(input,hidden)
            topv, topi = output.topk(1)
            topi = topi[0][0]
            if topi == n_letters - 1:
                break
            else:
                letter = all_letters[topi]
                output_name += letter

            input = name2tensor(letter).view(-1, 1, input_size)

        return output_name


# Get multiple samples from one category and multiple starting letters
def samples(start_letters='ABC'):
    for start_letter in start_letters:
        print(sample(start_letter))


for epoch in range(20):

    loss_sum = 0;
    nRound = 0
    for i_batch, batch in enumerate(dataloader):

        inputs = batch

        for idx, input in enumerate(inputs):
            label = input[1:] + all_letters[-1]

            optimizer.zero_grad()

            input = name2tensor(input).view(-1, 1, input_size)
            label = [all_letters.index(x) for x in label]

            output, _ = model(input,  None)
            loss = criterion(output, torch.LongTensor(label))

            loss.backward()
            optimizer.step()

            loss_sum += loss
            nRound += 1

    print("epoch {} i_batch {} loss {}".format(epoch, i_batch, loss_sum / nRound))
    samples()







