{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# This is the pytorch version of karpathy's charnn\n",
    "    reference: https://gist.github.com/raphaelbastide/11ae4bb5e454e5c5239f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has {} characters, {} unique.'.format( data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def ix_to_tensor(ix):\n",
    "    t = torch.FloatTensor(1, vocab_size).zero_()\n",
    "    t[0][ix] = 1.0\n",
    "    return Variable(t)\n",
    "\n",
    "def tensor_to_ix(t):\n",
    "    v,i = torch.max(t,1)\n",
    "    return i.data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# nn module\n",
    "class CharRNN(nn.Module):\n",
    " \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNNCell(input_size, hidden_size , nonlinearity='relu')\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.FloatTensor(1, self.hidden_size).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rnn = CharRNN(vocab_size, hidden_size,vocab_size )\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(inputs, targets, hprev):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = ix_to_tensor(inputs[i])\n",
    "        target = Variable(torch.LongTensor([targets[i]]))\n",
    "\n",
    "        output, hprev = rnn(input, hprev)\n",
    "        loss += criterion(output, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0] , hprev\n",
    "\n",
    "def sample(seed_ix, n):\n",
    "    h = rnn.init_hidden()\n",
    "\n",
    "    x = ix_to_tensor(seed_ix)\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        output, h = rnn(x, h)\n",
    "        ix = tensor_to_ix(output)\n",
    "        ixes.append(ix)\n",
    "        x = ix_to_tensor(ix)\n",
    "        \n",
    "    return ixes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "\n",
    "while True:\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = rnn.init_hidden()\n",
    "        p = 0 # go from start of data\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample( inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n {} \\n----'.format( txt) )\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss,hprev = train(inputs, targets, hprev) \n",
    "    hprev = Variable(hprev.data)\n",
    "    \n",
    " \n",
    "    if n % 1000 == 0:\n",
    "        print( 'iter {}, loss: {}'.format(n, loss)) # print progress\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
