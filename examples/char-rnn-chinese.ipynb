{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the pytorch version of karpathy's charnn\n",
    "    reference: https://gist.github.com/raphaelbastide/11ae4bb5e454e5c5239f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 391021 characters, 3477 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('input1.txt', 'r',encoding='utf-8').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has {} characters, {} unique.'.format( data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def ix_to_tensor(ix):\n",
    "    t = torch.FloatTensor(1, vocab_size).zero_()\n",
    "    t[0][ix] = 1.0\n",
    "    return Variable(t).cuda()\n",
    "\n",
    "def tensor_to_ix(t):\n",
    "    v,i = torch.max(t,1)\n",
    "    return i.data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn module\n",
    "class CharRNN(nn.Module):\n",
    " \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNNCell(input_size, hidden_size , nonlinearity='relu')\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.FloatTensor(1, self.hidden_size).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn = CharRNN(vocab_size, hidden_size,vocab_size ).cuda()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(inputs, targets, hprev):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = ix_to_tensor(inputs[i]).cuda()\n",
    "        target = Variable(torch.LongTensor([targets[i]])).cuda()\n",
    "\n",
    "        output, hprev = rnn(input, hprev)\n",
    "        loss += criterion(output, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0] , hprev\n",
    "\n",
    "def sample(seed_ix, n):\n",
    "    h = rnn.init_hidden().cuda()\n",
    "\n",
    "    x = ix_to_tensor(seed_ix)\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        output, h = rnn(x, h)\n",
    "        ix = tensor_to_ix(output)\n",
    "        ixes.append(ix)\n",
    "        x = ix_to_tensor(ix)\n",
    "        \n",
    "    return ixes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 谷谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷腻谷 \n",
      "----\n",
      "iter 0, loss: 204.08926391601562\n",
      "----\n",
      " ，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，， \n",
      "----\n",
      "iter 1000, loss: 145.5849609375\n",
      "----\n",
      " ，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我们是是是是是是是的，我 \n",
      "----\n",
      "iter 2000, loss: 134.4222869873047\n",
      "----\n",
      " ，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这个一个，我们这 \n",
      "----\n",
      "iter 3000, loss: 136.49705505371094\n",
      "----\n",
      " 么，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们不是一个，我们 \n",
      "----\n",
      "iter 4000, loss: 126.23468780517578\n",
      "----\n",
      " 个，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的人，我们的 \n",
      "----\n",
      "iter 5000, loss: 123.58552551269531\n",
      "----\n",
      " 我们的人，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的沙漠中的，我们的 \n",
      "----\n",
      "iter 6000, loss: 135.38330078125\n",
      "----\n",
      " 们的人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是一个人，我们这些人是 \n",
      "----\n",
      "iter 7000, loss: 95.65715026855469\n",
      "----\n",
      " “我们的人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么不是一个人，我们这么 \n",
      "----\n",
      "iter 8000, loss: 130.46707153320312\n",
      "----\n",
      " 是一个，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这里面的，这 \n",
      "----\n",
      "iter 9000, loss: 143.70431518554688\n",
      "----\n",
      " 己的，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大金牙，我们这么大 \n",
      "----\n",
      "iter 10000, loss: 123.53480529785156\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "\n",
    "while True:\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = rnn.init_hidden().cuda()\n",
    "        p = 0 # go from start of data\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample( inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n {} \\n----'.format( txt) )\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss,hprev = train(inputs, targets, hprev) \n",
    "    hprev = Variable(hprev.data)\n",
    "    \n",
    " \n",
    "    if n % 1000 == 0:\n",
    "        print( 'iter {}, loss: {}'.format(n, loss)) # print progress\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
